{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "not_fit = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(df['Page content'][2])\n",
    "\n",
    "#content = procd.parse_html(df['Page content'][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nch = np.zeros((df.values.shape[0], 33))\\n\\nfor i in df.values:\\n    key = procd.parse_channel(i[2])\\n    ch[a, procd.dic[key]] = 1\\n    a += 1\\n\\ncd = pd.DataFrame(data=ch)\\ncd.to_csv('train_channel.csv',index=False)\\n\\ndf_test = pd.read_csv('test.csv')\\ncn = np.zeros((df_test.values.shape[0], 33))\\na=0\\nfor i in df_test.values:\\n    key = procd.parse_channel(i[2])\\n    if key in dic:\\n        cn[a, dic[key]] = 1\\n        a += 1\\ntd = pd.DataFrame(data=cn)\\ntd.to_csv('test_channel.csv', index=False)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import src.proc_data as procd\n",
    "\"\"\"\n",
    "parse_train = np.zeros(df.values.shape, dtype=object)\n",
    "parse_test = np.zeros(df_test.values.shape, dtype=object)\n",
    "\n",
    "for i in range(df.values.shape[0]):\n",
    "    data = df.values[i]\n",
    "    content = procd.parse_html(data[2])\n",
    "    parse_train[i, 0:2] = data[0:2]\n",
    "    parse_train[i, 2] = content\n",
    "\n",
    "parse_df = pd.DataFrame(data=parse_train, columns=df.columns)\n",
    "parse_df.to_csv('train_parsed.csv', index=False)\n",
    "    \n",
    "for i in range(df_test.values.shape[0]):\n",
    "    data = df_test.values[i]\n",
    "    content = procd.parse_html(data[1])\n",
    "    parse_test[i, 0] = data[0]\n",
    "    parse_test[i, 1] = content\n",
    "\n",
    "parse_df_test = pd.DataFrame(data=parse_test, columns=df_test.columns)\n",
    "parse_df_test.to_csv('test_parsed.csv', index=False)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "for i in df.values:\n",
    "    topics = procd.parse_topic(i[2])\n",
    "    for t in topics:\n",
    "        dic[t] = dic.get(t, 0) + 1\n",
    "        if i[1] == 1: pop[t] = pop.get(t, 0) + 1\n",
    "print(dic)\n",
    "print(pop)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "a = 0\n",
    "train_times = np.zeros((df.values.shape[0], 8), dtype=float)\n",
    "test_times = np.zeros((df_test.values.shape[0], 8), dtype=float)\n",
    "\n",
    "for i in df.values:\n",
    "    text = BeautifulSoup(i[2], 'html.parser')\n",
    "    weekday, train_times[7] = procd.parse_date(text.find('time')['datetime'])\n",
    "    train_times[weekday] = 1\n",
    "\n",
    "for i in df_test.values:\n",
    "  \n",
    "    text = BeautifulSoup(i[1], 'html.parser')\n",
    "    try:\n",
    "        weekday, test_times[7] = procd.parse_date(text.find('time')['datetime'])\n",
    "    except:\n",
    "        weekday = 0\n",
    "        test_times[7] = 0\n",
    "    test_times[weekday] = 1\n",
    "cd = pd.DataFrame(data=train_times)\n",
    "cd.to_csv('train_times.csv',index=False)\n",
    "td = pd.DataFrame(data=test_times)\n",
    "td.to_csv('test_times.csv', index=False)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "ch = np.zeros((df.values.shape[0], 33))\n",
    "\n",
    "for i in df.values:\n",
    "    key = procd.parse_channel(i[2])\n",
    "    ch[a, procd.dic[key]] = 1\n",
    "    a += 1\n",
    "\n",
    "cd = pd.DataFrame(data=ch)\n",
    "cd.to_csv('train_channel.csv',index=False)\n",
    "\n",
    "df_test = pd.read_csv('test.csv')\n",
    "cn = np.zeros((df_test.values.shape[0], 33))\n",
    "a=0\n",
    "for i in df_test.values:\n",
    "    key = procd.parse_channel(i[2])\n",
    "    if key in dic:\n",
    "        cn[a, dic[key]] = 1\n",
    "        a += 1\n",
    "td = pd.DataFrame(data=cn)\n",
    "td.to_csv('test_channel.csv', index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_parsed.csv')\n",
    "df_test = pd.read_csv('test_parsed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/AtSu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "p_data = df[df['Popularity'] > 0] \n",
    "n_data = df[df['Popularity'] < 0]\n",
    "\n",
    "tokenizer_stem_nostop('amazon microsoft women hahahaha')\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools, operator\n",
    "\n",
    "def extract_term(top, tfidf):\n",
    "    feat = tfidf.get_feature_names()\n",
    "    idf = tfidf.idf_\n",
    "    idx = idf.argsort()\n",
    "    sorted_score = []\n",
    "    \n",
    "    for i in idx: sorted_score.append(idf[i])\n",
    "    sorted_smallest = list(zip(feat, sorted_score))\n",
    "    sorted_smallest = sorted(sorted_smallest, key=operator.itemgetter(1))\n",
    "    sorted_highest = sorted_smallest[::-1]\n",
    "    term = []\n",
    "    print('[vocabularies with smallest idf scores]')\n",
    "    for i in range(top):\n",
    "        #print('%s : %f' %(sorted_smallest[i][0], sorted_smallest[i][1]))\n",
    "        term.append(sorted_smallest[i][0])\n",
    "        \n",
    "    print('[vocabularies with height idf scores]')\n",
    "    for i in range(top):\n",
    "        #print('%s : %f' %(sorted_highest[i][0], sorted_highest[i][1]))\n",
    "        term.append(sorted_highest[i][0])\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "[vocabularies with height idf scores]\n",
      "[vocabularies with smallest idf scores]\n",
      "[vocabularies with height idf scores]\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn', 'abil', 'abl', 'access', 'accord', 'account', 'across', 'act', 'action', 'activ', 'actual', 'ad', 'add', 'addit', 'address', 'advanc', 'advertis', 'age', 'agenc', 'ago', 'aim', 'air', 'allow', 'almost', 'along', 'alreadi', 'although', 'alway', 'amaz', 'america', 'american', 'among', 'amount', 'android', 'anim', 'announc', 'anoth', 'answer', 'anyon', 'anyth', 'app', 'youtub', 'young', 'york', 'yet', 'year', 'wrote', 'write', 'would', 'worth', 'world', 'work', 'word', 'wonder', 'women', 'without', 'within', 'window', 'win', 'wide', 'whose', 'whole', 'white', 'whether', 'went', 'well', 'week', 'wednesday', 'websit', 'web', 'wear', 'way', 'water', 'watch', 'war', 'want', 'wall', 'walk', 'wait', 'voic']\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import _pickle as pkl\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "p = p_data['Page content']\n",
    "n = n_data['Page content']\n",
    "\n",
    "if not_fit:\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1,1),\n",
    "                        #preprocessor=procd.parse_html,\n",
    "                        tokenizer=tokenizer_stem_nostop,\n",
    "                        max_features=2**10,\n",
    "                        max_df=0.6,\n",
    "                        min_df=0.05,\n",
    "                        sublinear_tf=True\n",
    "                        )\n",
    "    not_fit = False\n",
    "    tfidf_p = tfidf.fit(p)\n",
    "    pkl.dump(tfidf_p, open('tfidf_p.pkl', 'wb'))\n",
    "    tfidf_n = tfidf.fit(n)\n",
    "    pkl.dump(tfidf_n, open('tfidf_n.pkl', 'wb'))\n",
    "\n",
    "tfidf_p = pkl.load(open('tfidf_p.pkl', 'rb'))\n",
    "tfidf_n = pkl.load(open('tfidf_n.pkl', 'rb'))\n",
    "\n",
    "t_p = extract_term(40, tfidf_p)\n",
    "t_n = extract_term(40, tfidf_n)\n",
    "   \n",
    "extra_stop = []\n",
    "for t in t_p:\n",
    "    if t in t_n: extra_stop.append(t)\n",
    "stop += extra_stop\n",
    "\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing fitting TFIDF : 180.395193 s\n",
      "dimension: 711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from time import time\n",
    "t0 = time()\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2),\n",
    "                        #preprocessor=procd.parse_html,\n",
    "                        tokenizer=tokenizer_stem_nostop,\n",
    "                        max_features=2**16,\n",
    "                        max_df=0.5,\n",
    "                        min_df=0.05,\n",
    "                        sublinear_tf=True\n",
    "                        )\n",
    "X_train = tfidf.fit_transform(df['Page content'])\n",
    "duration = time() - t0\n",
    "\n",
    "print('Finishing fitting TFIDF : %f s' %(duration))\n",
    "print('dimension: %d' %(X_train.shape[1]))\n",
    "#sc_x = MaxAbsScaler()\n",
    "#X_train = sc_x.fit_transform(X_train)\n",
    "y_train = df['Popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing fitting HASH : 151.906644 s\n",
      "dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "t0 = time()\n",
    "hashvec = HashingVectorizer(ngram_range=(1,2),\n",
    "                           tokenizer=tokenizer_stem_nostop,\n",
    "                           n_features=2**10)\n",
    "X_train = hashvec.transform(df['Page content'])\n",
    "duration = time() - t0\n",
    "print('Finishing fitting HASH : %f s' %(duration))\n",
    "print('dimension: %d' %(X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 8233)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "train_channel = pd.read_csv('train_channel.csv').values\n",
    "train_time = pd.read_csv('train_times.csv').values\n",
    "X_train = hstack([X_train, train_channel, train_time])\n",
    "#sc_x = StandardScaler(with_mean=False)\n",
    "#X_train = sc_x.fit_transform(X_train)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB score : 0.506638\n",
      "SGD score : 0.589914\n",
      "SGD score : 0.589625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfor C in Cs:\\n    alpha = C / 1000.0\\n    clf = SGDClassifier(loss='hinge', alpha=alpha)\\n    #stream = get_stream(path='train.csv', size=batch_size)\\n    for i in range(iters):\\n        #batch = next(stream)\\n        #X_train, y_train = batch['Page content'], batch['Popularity']\\n        start = (i)*batch_size\\n        end = start + batch_size\\n        if end >= X_train.shape[0] : end = X_train.shape[0]\\n        \\n        batch_X = X_train[start:end]\\n        batch_y = y_train[start:end]\\n        if X_train is None:\\n            break\\n        #X_train = sc_x.transform(tfidf.transform(X_train))\\n        clf.partial_fit(batch_X, batch_y, classes=classes)\\n    \\n        score = clf.score(batch_X, batch_y)\\n        print('[{}/{}] {}'.format((i+1)*(batch_size), 27643, score))\\n    \\n    if score > best_score:\\n        print('bestscore(C=%f): %f' %(C, score))\\n        best_score = score\\n        candidates.append({'clf' : clf, 'C' : C, 'score' : score})\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "\n",
    "batch_size = 2000\n",
    "\n",
    "classes = np.array([-1, 1])\n",
    "iters = int((27643+batch_size-1)/(batch_size))\n",
    "Cs = [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 3.0, 5.0, 7.0, 10.0]\n",
    "\n",
    "best_score = 0\n",
    "candidates = []\n",
    "\"\"\"\n",
    "t0 = time()\n",
    "clf = SVC(max_iter=15000, random_state=0)\n",
    "clf.fit(X_train, df['Popularity'])\n",
    "candidates.append({'clf':clf, 'C':C, 'score':score})\n",
    "score = clf.score(X_train, df['Popularity'])\n",
    "print(score)\n",
    "candidates.append({'clf':clf, 'C':0.001, 'score':score})\n",
    "print('training duration : %f' %(time()-t0))\n",
    "\"\"\"\n",
    "\n",
    "#selector = SelectFromModel(estimator=LinearSVC(dual=False, tol=1e-3))\n",
    "#X_train_sel = selector.fit_transform(X_train, y_train)\n",
    "X_train_sel = X_train\n",
    "#poly = PolynomialFeatures(degree=3)\n",
    "#X_train_poly = poly.fit_transform(X_train_sel.toarray())\n",
    "X_train_poly = X_train_sel\n",
    "clf = MultinomialNB(alpha=0.01)\n",
    "clf.fit(abs(X_train_sel), y_train)\n",
    "pred = clf.predict(X_train_sel)\n",
    "\n",
    "print(\"MNB score : %f\" %(metrics.accuracy_score(y_train, pred)))\n",
    "candidates.append({'clf':clf, 'C':0, 'score':score})\n",
    "\n",
    "SGD = SGDClassifier(alpha=0.0001, loss='hinge', penalty='l2')\n",
    "SGD.fit(X_train_sel, y_train)\n",
    "pred = SGD.predict(X_train_sel)\n",
    "\n",
    "print(\"SGD score : %f\" %(metrics.accuracy_score(y_train, pred)))\n",
    "candidates.append({'clf':SGD, 'C':1, 'score':score})\n",
    "\n",
    "\n",
    "ELA = SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
    "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
    "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
    "       verbose=0, warm_start=False)\n",
    "ELA.fit(X_train_sel, y_train)\n",
    "pred = ELA.predict(X_train_sel)\n",
    "\n",
    "print(\"SGD score : %f\" %(metrics.accuracy_score(y_train, pred)))\n",
    "candidates.append({'clf':ELA, 'C':2, 'score':score})\n",
    "\n",
    "\"\"\"\n",
    "for C in Cs:\n",
    "    clf = Pipeline([\n",
    "            ('feature_selection', SelectFromModel(estimator=LinearSVC(penalty='l1', dual=False, tol=1e-3))),\n",
    "            ('classification', LinearSVC(C=C))\n",
    "        ])\n",
    "    #clf = LinearSVC(C=C, dual=False, tol=0.00001, max_iter=2000)\n",
    "    clf.fit(X_train_sel, y_train)\n",
    "    pred = clf.predict(X_train_sel)\n",
    "\n",
    "    score = metrics.accuracy_score(y_train, pred)\n",
    "    print(\"C(%f):%f\"%(C,score))\n",
    "    candidates.append({'clf':clf, 'C':C, 'score':score})\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "for C in Cs:\n",
    "    alpha = C / 1000.0\n",
    "    clf = SGDClassifier(loss='hinge', alpha=alpha)\n",
    "    #stream = get_stream(path='train.csv', size=batch_size)\n",
    "    for i in range(iters):\n",
    "        #batch = next(stream)\n",
    "        #X_train, y_train = batch['Page content'], batch['Popularity']\n",
    "        start = (i)*batch_size\n",
    "        end = start + batch_size\n",
    "        if end >= X_train.shape[0] : end = X_train.shape[0]\n",
    "        \n",
    "        batch_X = X_train[start:end]\n",
    "        batch_y = y_train[start:end]\n",
    "        if X_train is None:\n",
    "            break\n",
    "        #X_train = sc_x.transform(tfidf.transform(X_train))\n",
    "        clf.partial_fit(batch_X, batch_y, classes=classes)\n",
    "    \n",
    "        score = clf.score(batch_X, batch_y)\n",
    "        print('[{}/{}] {}'.format((i+1)*(batch_size), 27643, score))\n",
    "    \n",
    "    if score > best_score:\n",
    "        print('bestscore(C=%f): %f' %(C, score))\n",
    "        best_score = score\n",
    "        candidates.append({'clf' : clf, 'C' : C, 'score' : score})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'selector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-90f31845a2e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'selector' is not defined"
     ]
    }
   ],
   "source": [
    "print(selector.get_support())\n",
    "c = 0\n",
    "for i in selector.get_support():\n",
    "    if i == True: c += 1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing fitting test TFIDF : 63.737878 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "X_test = tfidf.transform(df_test['Page content'])\n",
    "duration = time() - t0\n",
    "\n",
    "print('Finishing fitting test TFIDF : %f s' %(duration))\n",
    "#X_test = sc_x.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing fitting test Hashvec : 63.491124 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "X_test = hashvec.transform(df_test['Page content'])\n",
    "duration = time() - t0\n",
    "print('Finishing fitting test Hashvec : %f s' %(duration))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_channel = pd.read_csv('test_channel.csv').values\n",
    "test_time = pd.read_csv('test_times.csv').values\n",
    "X_test_con = hstack([X_test, test_channel, test_time])\n",
    "\n",
    "#X_test_std = sc_x.fit_transform(X_test_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_con = X_test\n",
    "\n",
    "for cand in candidates:\n",
    "    predict = cand['clf'].predict(X_test_con)\n",
    "    output = np.zeros((X_test_con.shape[0], 2), dtype=int)\n",
    "    output[:, 0] = df_test['Id']\n",
    "    output[:, 1] = predict\n",
    "    df_output = pd.DataFrame(data=output, columns=['Id', 'Popularity'])\n",
    "    df_output.to_csv('test%f.csv' % (cand['C']), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ML_ENV]",
   "language": "python",
   "name": "conda-env-ML_ENV-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
