{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "not_fit = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head><div class=\"article-info\"><span class=\"byline \"><a href=\"/author/christina/\"><img alt=\"2016%2f06%2f28%2fa2%2fhttpsd2mhye01h4nj2n.cloudfront.netmediazgkymde1lza0.6e864\" class=\"author_image\" src=\"http://i.amz.mshcdn.com/EOF74clpv5lKQmem8EbdqygnSIA=/90x90/2016%2F06%2F28%2Fa2%2Fhttpsd2mhye01h4nj2n.cloudfront.netmediaZgkyMDE1LzA0.6e864.jpg\"/></a><span class=\"author_name\">By <a href=\"/author/christina/\">Christina Warren</a></span><time datetime=\"Thu, 28 Mar 2013 17:40:55 +0000\">2013-03-28 17:40:55 UTC</time></span></div></head><body><h1 class=\"title\">Google's New Open Source Patent Pledge: We Won't Sue Unless Attacked First</h1><figure class=\"article-image\"><img class=\"microcontent\" data-fragment=\"lead-image\" data-image=\"http://i.amz.mshcdn.com/libxaUsE6Ziezz1FVi_31dND9gs=/950x534/2013%2F03%2F28%2Fd6%2Fpatentmagni.f1555.jpg\" data-micro=\"1\" data-url=\"null\" src=\"http://i.amz.mshcdn.com/libxaUsE6Ziezz1FVi_31dND9gs=/950x534/2013%2F03%2F28%2Fd6%2Fpatentmagni.f1555.jpg\"/></figure><article data-channel=\"tech\"><section class=\"article-content\"> <p>Google took a stand of sorts against <a href=\"http://mashable.com/category/patent-lawsuit-theater/\">patent-lawsuit theater</a> Thursday with its new <a href=\"http://www.google.com/patents/opnpledge/\" target=\"_blank\">Open Patent Non-Assertion (OPN) Pledge</a>. </p> <p>As <a href=\"http://google-opensource.blogspot.co.uk/2013/03/taking-stand-on-open-source-and-patents.html\" target=\"_blank\">explained by Google's Duane Valz</a>, under the OPN Pledge, Google promises \"not to sue any user, distributor or developer of open-source software on specified patents, unless first attacked.\"</p> <p>Now, Google isn't making all of its patents available for others. Instead, its starting small with <a href=\"http://www.google.com/patents/opnpledge/patents/\" target=\"_blank\">10 patents</a> focused on MapReduce, a programming model for handling large data sets. There are already open-sourced versions of MapReduce available — including Hadoop — that are widely used across the Internet. Google says that over time, it plans to extend the OPN Pledge to more Google patents.</p> <p><strong></strong></p> <p>The big caveat here is that Google is pledging not to sue anyone who uses its MapReduce patents for Free or Open Source Software. Google is <a href=\"http://www.google.com/patents/opnpledge/pledge/\" target=\"_blank\">defining Free or Open Source software</a> as any software that meets the Open Source Initiative's \"Open Source Definition,\" as well as any version of the Free Software Foundation's \"Free Software Definition.\" Still, Google iterates that the OPN Pledge isn't limited to a specific project or open-source copyright — as long as the project meets the FSF or OSI's definition for Free Software or Open Source Software, it's protected by the OPN Pledge.</p> <p>Google hopes that its OPN Pledge can be a model for other companies who are looking at how to \"put their own patents into the service of open-source software.\"</p> <p>In the short-term, we're not sure what this does — except indemnify the open source MapReduce projects from a Google lawsuit. The bigger picture, however, is that this could be a new model for the way that patents are applied to open source as a whole. And that's a good thing.</p> <p><em>Image courtesy of <a href=\"http://www.istockphoto.com/mashableoffer.php\" target=\"_blank\">iStockphoto</a>, <a href=\"http://www.istockphoto.com/user_view.php?id=3135499\" target=\"_blank\">stuartbur</a></em></p> </section></article><footer class=\"article-topics\"> Topics: <a href=\"/category/apps-software/\">Apps and Software</a>, <a href=\"/category/google/\">Google</a>, <a href=\"/category/open-source/\">open source</a>, <a href=\"/category/opn-pledge/\">opn pledge</a>, <a href=\"/category/patent-lawsuit-theater/\">patent lawsuit theater</a>, <a href=\"/category/software-patents/\">software patents</a>, <a href=\"/category/tech/\">Tech</a>, <a href=\"/category/us/\">U.S.</a> </footer></body></html>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = BeautifulSoup(data[1], 'html.parser')\n",
    "text.find('div', attrs={'class':'see-also'}).decompose()\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nch = np.zeros((df.values.shape[0], 33))\\n\\nfor i in df.values:\\n    key = procd.parse_channel(i[2])\\n    ch[a, procd.dic[key]] = 1\\n    a += 1\\n\\ncd = pd.DataFrame(data=ch)\\ncd.to_csv('train_channel.csv',index=False)\\n\\ndf_test = pd.read_csv('test.csv')\\ncn = np.zeros((df_test.values.shape[0], 33))\\na=0\\nfor i in df_test.values:\\n    key = procd.parse_channel(i[2])\\n    if key in dic:\\n        cn[a, dic[key]] = 1\\n        a += 1\\ntd = pd.DataFrame(data=cn)\\ntd.to_csv('test_channel.csv', index=False)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import src.proc_data as procd\n",
    "\"\"\"\n",
    "dic = {}\n",
    "pop = {}\n",
    "for i in df.values:\n",
    "    topics = procd.parse_topic(i[2])\n",
    "    for t in topics:\n",
    "        dic[t] = dic.get(t, 0) + 1\n",
    "        if i[1] == 1: pop[t] = pop.get(t, 0) + 1\n",
    "print(dic)\n",
    "print(pop)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "a = 0\n",
    "train_times = np.zeros((df.values.shape[0], 8), dtype=float)\n",
    "test_times = np.zeros((df_test.values.shape[0], 8), dtype=float)\n",
    "\n",
    "for i in df.values:\n",
    "    text = BeautifulSoup(i[2], 'html.parser')\n",
    "    weekday, train_times[7] = procd.parse_date(text.find('time')['datetime'])\n",
    "    train_times[weekday] = 1\n",
    "\n",
    "for i in df_test.values:\n",
    "  \n",
    "    text = BeautifulSoup(i[1], 'html.parser')\n",
    "    try:\n",
    "        weekday, test_times[7] = procd.parse_date(text.find('time')['datetime'])\n",
    "    except:\n",
    "        weekday = 0\n",
    "        test_times[7] = 0\n",
    "    test_times[weekday] = 1\n",
    "cd = pd.DataFrame(data=train_times)\n",
    "cd.to_csv('train_times.csv',index=False)\n",
    "td = pd.DataFrame(data=test_times)\n",
    "td.to_csv('test_times.csv', index=False)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "ch = np.zeros((df.values.shape[0], 33))\n",
    "\n",
    "for i in df.values:\n",
    "    key = procd.parse_channel(i[2])\n",
    "    ch[a, procd.dic[key]] = 1\n",
    "    a += 1\n",
    "\n",
    "cd = pd.DataFrame(data=ch)\n",
    "cd.to_csv('train_channel.csv',index=False)\n",
    "\n",
    "df_test = pd.read_csv('test.csv')\n",
    "cn = np.zeros((df_test.values.shape[0], 33))\n",
    "a=0\n",
    "for i in df_test.values:\n",
    "    key = procd.parse_channel(i[2])\n",
    "    if key in dic:\n",
    "        cn[a, dic[key]] = 1\n",
    "        a += 1\n",
    "td = pd.DataFrame(data=cn)\n",
    "td.to_csv('test_channel.csv', index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/atsu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "p_data = df[df['Popularity'] > 0] \n",
    "n_data = df[df['Popularity'] < 0]\n",
    "\n",
    "tokenizer_stem_nostop('amazon microsoft women hahahaha')\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools, operator\n",
    "\n",
    "def extract_term(top, tfidf):\n",
    "    feat = tfidf.get_feature_names()\n",
    "    idf = tfidf.idf_\n",
    "    idx = idf.argsort()\n",
    "    sorted_score = []\n",
    "    \n",
    "    for i in idx: sorted_score.append(idf[i])\n",
    "    sorted_smallest = list(zip(feat, sorted_score))\n",
    "    sorted_smallest = sorted(sorted_smallest, key=operator.itemgetter(1))\n",
    "    sorted_highest = sorted_smallest[::-1]\n",
    "    term = []\n",
    "    print('[vocabularies with smallest idf scores]')\n",
    "    for i in range(top):\n",
    "        #print('%s : %f' %(sorted_smallest[i][0], sorted_smallest[i][1]))\n",
    "        term.append(sorted_smallest[i][0])\n",
    "        \n",
    "    print('[vocabularies with height idf scores]')\n",
    "    for i in range(top):\n",
    "        #print('%s : %f' %(sorted_highest[i][0], sorted_highest[i][1]))\n",
    "        term.append(sorted_highest[i][0])\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-c77b9ca1ad5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                         )\n\u001b[1;32m     18\u001b[0m     \u001b[0mnot_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtfidf_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf_p.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtfidf_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/AtSu/anaconda/envs/ML_ENV/lib/python3.4/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \"\"\"\n\u001b[0;32m-> 1312\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/AtSu/anaconda/envs/ML_ENV/lib/python3.4/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 824\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/AtSu/anaconda/envs/ML_ENV/lib/python3.4/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/AtSu/anaconda/envs/ML_ENV/lib/python3.4/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d21c8c81f7b4>\u001b[0m in \u001b[0;36mtokenizer_stem_nostop\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenizer_stem_nostop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mporter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\s+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[a-zA-Z]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-d21c8c81f7b4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenizer_stem_nostop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mporter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\s+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[a-zA-Z]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/AtSu/anaconda/envs/ML_ENV/lib/python3.4/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;31m## Define a stem() method that implements the StemmerI interface.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adjust_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/AtSu/anaconda/envs/ML_ENV/lib/python3.4/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36mstem_word\u001b[0;34m(self, p, i, j)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/AtSu/anaconda/envs/ML_ENV/lib/python3.4/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_step4\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'n'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ement\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import _pickle as pkl\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "p = p_data['Page content']\n",
    "n = n_data['Page content']\n",
    "\n",
    "if not_fit:\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1,2),\n",
    "                        preprocessor=procd.parse_html,\n",
    "                        tokenizer=tokenizer_stem_nostop,\n",
    "                        max_features=1024,\n",
    "                        max_df=0.5\n",
    "                        )\n",
    "    not_fit = False\n",
    "    tfidf_p = tfidf.fit(p)\n",
    "    pkl.dump(tfidf_p, open('tfidf_p.pkl', 'wb'))\n",
    "    tfidf_n = tfidf.fit(n)\n",
    "    pkl.dump(tfidf_n, open('tfidf_n.pkl', 'wb'))\n",
    "\n",
    "tfidf_p = pkl.load(open('tfidf_p.pkl', 'rb'))\n",
    "tfidf_n = pkl.load(open('tfidf_n.pkl', 'rb'))\n",
    "\n",
    "t_p = extract_term(40, tfidf_p)\n",
    "t_n = extract_term(40, tfidf_n)\n",
    "   \n",
    "extra_stop = []\n",
    "for t in t_p:\n",
    "    if t in t_n: extra_stop.append(t)\n",
    "stop += extra_stop\n",
    "\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing fitting TFIDF : 341.380511 s\n",
      "dimension: 8192\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from time import time\n",
    "t0 = time()\n",
    "tfidf = TfidfVectorizer(\n",
    "                        preprocessor=procd.parse_html,\n",
    "                        tokenizer=tokenizer_stem_nostop,\n",
    "                        max_features=2**13,\n",
    "                        max_df=0.5,\n",
    "                        sublinear_tf=True\n",
    "                        )\n",
    "X_train = tfidf.fit_transform(df['Page content'])\n",
    "duration = time() - t0\n",
    "\n",
    "print('Finishing fitting TFIDF : %f s' %(duration))\n",
    "print('dimension: %d' %(X_train.shape[1]))\n",
    "#sc_x = MaxAbsScaler()\n",
    "#X_train = sc_x.fit_transform(X_train)\n",
    "y_train = df['Popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 8233)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "train_channel = pd.read_csv('train_channel.csv').values\n",
    "train_time = pd.read_csv('train_times.csv').values\n",
    "X_train = hstack([X_train, train_channel, train_time])\n",
    "#sc_x = StandardScaler(with_mean=False)\n",
    "#X_train = sc_x.fit_transform(X_train)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C(0.100000):0.687588\n",
      "C(0.500000):0.723366\n",
      "C(1.000000):0.730348\n",
      "C(2.000000):0.735014\n",
      "C(3.000000):0.736751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfor C in Cs:\\n    alpha = C / 1000.0\\n    clf = SGDClassifier(loss='hinge', alpha=alpha)\\n    #stream = get_stream(path='train.csv', size=batch_size)\\n    for i in range(iters):\\n        #batch = next(stream)\\n        #X_train, y_train = batch['Page content'], batch['Popularity']\\n        start = (i)*batch_size\\n        end = start + batch_size\\n        if end >= X_train.shape[0] : end = X_train.shape[0]\\n        \\n        batch_X = X_train[start:end]\\n        batch_y = y_train[start:end]\\n        if X_train is None:\\n            break\\n        #X_train = sc_x.transform(tfidf.transform(X_train))\\n        clf.partial_fit(batch_X, batch_y, classes=classes)\\n    \\n        score = clf.score(batch_X, batch_y)\\n        print('[{}/{}] {}'.format((i+1)*(batch_size), 27643, score))\\n    \\n    if score > best_score:\\n        print('bestscore(C=%f): %f' %(C, score))\\n        best_score = score\\n        candidates.append({'clf' : clf, 'C' : C, 'score' : score})\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "batch_size = 2000\n",
    "\n",
    "classes = np.array([-1, 1])\n",
    "iters = int((27643+batch_size-1)/(batch_size))\n",
    "Cs = [0.1, 0.5, 1.0, 2.0, 3.0]\n",
    "\n",
    "best_score = 0\n",
    "candidates = []\n",
    "\"\"\"\n",
    "t0 = time()\n",
    "clf = SVC(max_iter=15000, random_state=0)\n",
    "clf.fit(X_train, df['Popularity'])\n",
    "candidates.append({'clf':clf, 'C':C, 'score':score})\n",
    "score = clf.score(X_train, df['Popularity'])\n",
    "print(score)\n",
    "candidates.append({'clf':clf, 'C':0.001, 'score':score})\n",
    "print('training duration : %f' %(time()-t0))\n",
    "\"\"\"\n",
    "\n",
    "#selector = SelectFromModel(estimator=LinearSVC(dual=False, tol=1e-3))\n",
    "#X_train_sel = selector.fit_transform(X_train, y_train)\n",
    "X_train_sel = X_train\n",
    "#poly = PolynomialFeatures(degree=3)\n",
    "#X_train_poly = poly.fit_transform(X_train_sel.toarray())\n",
    "X_train_poly = X_train_sel\n",
    "for C in Cs:\n",
    "    clf = Pipeline([\n",
    "            ('feature_selection', SelectFromModel(estimator=LinearSVC(penalty='l1', dual=False, tol=1e-3))),\n",
    "            ('classification', LinearSVC(C=C))\n",
    "        ])\n",
    "    #clf = LinearSVC(C=C, dual=False, tol=0.00001, max_iter=2000)\n",
    "    clf.fit(X_train_sel, y_train)\n",
    "    pred = clf.predict(X_train_sel)\n",
    "\n",
    "    score = metrics.accuracy_score(y_train, pred)\n",
    "    print(\"C(%f):%f\"%(C,score))\n",
    "    candidates.append({'clf':clf, 'C':C, 'score':score})\n",
    "\n",
    "\"\"\"\n",
    "for C in Cs:\n",
    "    alpha = C / 1000.0\n",
    "    clf = SGDClassifier(loss='hinge', alpha=alpha)\n",
    "    #stream = get_stream(path='train.csv', size=batch_size)\n",
    "    for i in range(iters):\n",
    "        #batch = next(stream)\n",
    "        #X_train, y_train = batch['Page content'], batch['Popularity']\n",
    "        start = (i)*batch_size\n",
    "        end = start + batch_size\n",
    "        if end >= X_train.shape[0] : end = X_train.shape[0]\n",
    "        \n",
    "        batch_X = X_train[start:end]\n",
    "        batch_y = y_train[start:end]\n",
    "        if X_train is None:\n",
    "            break\n",
    "        #X_train = sc_x.transform(tfidf.transform(X_train))\n",
    "        clf.partial_fit(batch_X, batch_y, classes=classes)\n",
    "    \n",
    "        score = clf.score(batch_X, batch_y)\n",
    "        print('[{}/{}] {}'.format((i+1)*(batch_size), 27643, score))\n",
    "    \n",
    "    if score > best_score:\n",
    "        print('bestscore(C=%f): %f' %(C, score))\n",
    "        best_score = score\n",
    "        candidates.append({'clf' : clf, 'C' : C, 'score' : score})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False ..., False False False]\n",
      "1729\n"
     ]
    }
   ],
   "source": [
    "print(selector.get_support())\n",
    "c = 0\n",
    "for i in selector.get_support():\n",
    "    if i == True: c += 1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing fitting test TFIDF : 147.352715 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "X_test = tfidf.transform(df_test['Page content'])\n",
    "duration = time() - t0\n",
    "\n",
    "print('Finishing fitting test TFIDF : %f s' %(duration))\n",
    "#X_test = sc_x.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_channel = pd.read_csv('test_channel.csv').values\n",
    "test_time = pd.read_csv('test_times.csv').values\n",
    "X_test_con = hstack([X_test, test_channel, test_time])\n",
    "\n",
    "#X_test_std = sc_x.fit_transform(X_test_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cand in candidates:\n",
    "    predict = cand['clf'].predict(X_test_con)\n",
    "    output = np.zeros((X_test_con.shape[0], 2), dtype=int)\n",
    "    output[:, 0] = df_test['Id']\n",
    "    output[:, 1] = predict\n",
    "    df_output = pd.DataFrame(data=output, columns=['Id', 'Popularity'])\n",
    "    df_output.to_csv('test%f.csv' % (cand['C']), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11847, 345)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ML_ENV]",
   "language": "python",
   "name": "conda-env-ML_ENV-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
