{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "not_fit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/AtSu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import src.proc_data as procd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazon', 'microsoft', 'women', 'hahahaha']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_data = df[df['Popularity'] > 0] \n",
    "n_data = df[df['Popularity'] < 0]\n",
    "\n",
    "tokenizer_stem_nostop('amazon microsoft women hahahaha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools, operator\n",
    "\n",
    "def extract_term(top, tfidf):\n",
    "    feat = tfidf.get_feature_names()\n",
    "    idf = tfidf.idf_\n",
    "    idx = idf.argsort()\n",
    "    sorted_score = []\n",
    "    \n",
    "    for i in idx: sorted_score.append(idf[i])\n",
    "    sorted_smallest = list(zip(feat, sorted_score))\n",
    "    sorted_smallest = sorted(sorted_smallest, key=operator.itemgetter(1))\n",
    "    sorted_highest = sorted_smallest[::-1]\n",
    "    term = []\n",
    "    print('[vocabularies with smallest idf scores]')\n",
    "    for i in range(top):\n",
    "        #print('%s : %f' %(sorted_smallest[i][0], sorted_smallest[i][1]))\n",
    "        term.append(sorted_smallest[i][0])\n",
    "        \n",
    "    print('[vocabularies with height idf scores]')\n",
    "    for i in range(top):\n",
    "        #print('%s : %f' %(sorted_highest[i][0], sorted_highest[i][1]))\n",
    "        term.append(sorted_highest[i][0])\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "[vocabularies with height idf scores]\n",
      "[vocabularies with smallest idf scores]\n",
      "[vocabularies with height idf scores]\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn', 'abl', 'access', 'accord', 'account', 'across', 'action', 'activ', 'actual', 'ad', 'add', 'addit', 'air', 'allow', 'along', 'alreadi', 'also', 'alway', 'amazon', 'american', 'among', 'android', 'anim', 'announc', 'anoth', 'app', 'appear', 'appl', 'april', 'area', 'around', 'articl', 'ask', 'associ', 'attack', 'author', 'avail', 'away', 'back', 'base', 'becom', 'begin', 'behind', 'believ', 'best', 'better', 'big', 'billion', 'bit', 'black', 'bonu', 'book', 'brand', 'break', 'bring', 'build', 'busi', 'buy', 'call', 'came', 'camera', 'campaign', 'car', 'card', 'case', 'caus', 'celebr', 'center', 'ceo', 'challeng', 'chang', 'charact', 'charg', 'check', 'citi', 'claim', 'close', 'co', 'collect', 'color', 'com', 'come', 'comment', 'commun', 'compani', 'complet', 'comput', 'connect', 'consid', 'consum', 'content', 'continu', 'control', 'cost', 'could', 'countri', 'courtesi', 'cover', 'creat', 'cup', 'current', 'custom', 'data', 'date', 'day', 'deal', 'design', 'detail', 'develop', 'devic', 'differ', 'digit', 'direct', 'youtub', 'york', 'yet', 'year old', 'year', 'write', 'would', 'world', 'work', 'word', 'women', 'without', 'within', 'window', 'win', 'white', 'whether', 'well', 'week', 'wednesday', 'websit', 'web', 'way', 'water', 'watch', 'war', 'want', 'view', 'video', 'via', 'version', 'user', 'use', 'us', 'updat', 'univers', 'unit', 'u', 'type', 'two', 'twitter com', 'twitter', 'tweet', 'tv', 'turn', 'tuesday', 'tri', 'travel', 'track', 'touch', 'top', 'tool', 'took', 'told', 'togeth', 'today', 'time', 'thursday', 'three', 'thought', 'though', 'think', 'thing', 'test', 'tell', 'technolog', 'tech', 'team', 'talk', 'take', 'tablet', 'system', 'sure', 'support', 'success', 'studi', 'student', 'street', 'stream', 'stori', 'store', 'stop', 'still', 'step', 'statement', 'state', 'startup', 'start', 'star', 'stand', 'sport', 'specif', 'space', 'sound', 'song', 'someth', 'someon', 'softwar', 'social media', 'social', 'smartphon', 'small', 'site', 'sinc', 'similar', 'sign', 'side', 'show', 'shot', 'share', 'sever', 'set', 'servic', 'seri', 'send', 'seen']\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import _pickle as pkl\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "p = p_data['Page content']\n",
    "n = n_data['Page content']\n",
    "\n",
    "if not_fit:\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1,3),\n",
    "                        preprocessor=procd.parse_html,\n",
    "                        tokenizer=tokenizer_stem_nostop,\n",
    "                        max_features=512,\n",
    "                        )\n",
    "    not_fit = False\n",
    "    tfidf_p = tfidf.fit(p)\n",
    "    pkl.dump(tfidf_p, open('tfidf_p.pkl', 'wb'))\n",
    "    tfidf_n = tfidf.fit(n)\n",
    "    pkl.dump(tfidf_n, open('tfidf_n.pkl', 'wb'))\n",
    "\n",
    "tfidf_p = pkl.load(open('tfidf_p.pkl', 'rb'))\n",
    "tfidf_n = pkl.load(open('tfidf_n.pkl', 'rb'))\n",
    "\n",
    "t_p = extract_term(120, tfidf_p)\n",
    "t_n = extract_term(120, tfidf_n)\n",
    "   \n",
    "extra_stop = []\n",
    "for t in t_p:\n",
    "    if t in t_n: extra_stop.append(t)\n",
    "stop += extra_stop\n",
    "\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=128, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2',\n",
       "        preprocessor=<function parse_html at 0x10c5bad90>, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function tokenizer_stem_nostop at 0x14eb48510>,\n",
       "        use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2),\n",
    "                        preprocessor=procd.parse_html,\n",
    "                        tokenizer=tokenizer_stem_nostop,\n",
    "                        max_features=128,\n",
    "                        )\n",
    "tfidf.fit(df['Page content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000/27643] 0.5885\n",
      "[4000/27643] 0.5975\n",
      "[6000/27643] 0.547\n",
      "[8000/27643] 0.5735\n",
      "[10000/27643] 0.542\n",
      "[12000/27643] 0.5585\n",
      "[14000/27643] 0.562\n",
      "[16000/27643] 0.549\n",
      "[18000/27643] 0.5375\n",
      "[20000/27643] 0.565\n",
      "[22000/27643] 0.549\n",
      "[24000/27643] 0.558\n",
      "[26000/27643] 0.5395\n",
      "[28000/27643] 0.5459525258673159\n",
      "bestscore(C=1.000000): 0.545953\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2000\n",
    "\n",
    "classes = np.array([-1, 1])\n",
    "iters = int((27643+batch_size-1)/(batch_size))\n",
    "Cs = [1.0]\n",
    "\n",
    "best_score = 0\n",
    "candidates = []\n",
    "\n",
    "\n",
    "\n",
    "for C in Cs:\n",
    "    clf = SGDClassifier(loss='hinge', alpha=1e-3)\n",
    "    stream = get_stream(path='train.csv', size=batch_size)\n",
    "    for i in range(iters):\n",
    "        batch = next(stream)\n",
    "        X_train, y_train = batch['Page content'], batch['Popularity']\n",
    "        if X_train is None:\n",
    "            break\n",
    "        X_train = tfidf.transform(X_train)\n",
    "        clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    \n",
    "        score = clf.score(X_train, y_train)\n",
    "        print('[{}/{}] {}'.format((i+1)*(batch_size), 27643, score))\n",
    "    \n",
    "    if score > best_score:\n",
    "        print('bestscore(C=%f): %f' %(C, score))\n",
    "        best_score = score\n",
    "        candidates.append({'clf' : clf, 'C' : C, 'score' : score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "X_test = tfidf.transform(df_test['Page content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cand in candidates:\n",
    "    predict = cand['clf'].predict(X_test)\n",
    "    output = np.zeros((X_test.shape[0], 2), dtype=int)\n",
    "    output[:, 0] = df_test['Id']\n",
    "    output[:, 1] = predict\n",
    "    df_output = pd.DataFrame(data=output, columns=['Id', 'Popularity'])\n",
    "    df_output.to_csv('test%f.csv' % (cand['C']), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ML_ENV]",
   "language": "python",
   "name": "conda-env-ML_ENV-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
