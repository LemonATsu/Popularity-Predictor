{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "not_fit = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/AtSu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_parse = np.zeros((df.values.shape[0], 5),dtype=object)\\n\\nfor i in range(df['Page content'].values.shape[0]):\\n#for i in range(5):\\n\\n    train_parse[i, 0], train_parse[i, 1], train_parse[i, 2],  train_parse[i, 3]            = procd.parse_html(df['Page content'][i])\\n    train_parse[i, 4] = len(tokenizer_stem_nostop(train_parse[i, 1]))\\n\\npd.DataFrame(data=train_parse, columns=['title', 'content', 'author', 'n_fig', 'n_words']).to_csv('train_b_parsed.csv')\\n\\ntrain_parse = np.zeros((df_test.values.shape[0], 5),dtype=object)\\n\\n#for i in range(5):\\n\\nfor i in range(df_test['Page content'].values.shape[0]):\\n    train_parse[i, 0], train_parse[i, 1], train_parse[i, 2],  train_parse[i, 3]             = procd.parse_html(df_test['Page content'][i])\\n    train_parse[i, 4] = len(tokenizer_stem_nostop(train_parse[i, 1]))\\n\\npd.DataFrame(data=train_parse, columns=['title', 'content', 'author','n_fig', 'n_words']).to_csv('test_b_parsed.csv')\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import src.proc_data as procd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\"\"\"\n",
    "train_channel = np.zeros((df.values.shape[0], 4), dtype=object)\n",
    "test_channel = np.zeros((df_test.values.shape[0], 4), dtype=object)\n",
    "columns = ['entertainment', 'world', 'social-media', 'tech']\n",
    "\n",
    "\n",
    "for i in range(df['Page content'].values.shape[0]):\n",
    "    channel = procd.parse_channel(df['Page content'][i])\n",
    "    if channel in columns:\n",
    "        train_channel[i, columns.index(channel)] = 1\n",
    "\n",
    "pd.DataFrame(data=train_channel, columns=columns).to_csv('train_channel.csv', index=False)\n",
    "        \n",
    "for i in range(df_test['Page content'].values.shape[0]):\n",
    "    channel = procd.parse_channel(df_test['Page content'][i])\n",
    "    if channel in columns:\n",
    "        test_channel[i, columns.index(channel)] = 1\n",
    "        \n",
    "pd.DataFrame(data=test_channel, columns=columns).to_csv('test_channel.csv', index=False)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "train_parse = np.zeros((df.values.shape[0], 5),dtype=object)\n",
    "\n",
    "for i in range(df['Page content'].values.shape[0]):\n",
    "#for i in range(5):\n",
    "\n",
    "    train_parse[i, 0], train_parse[i, 1], train_parse[i, 2],  train_parse[i, 3]\\\n",
    "            = procd.parse_html(df['Page content'][i])\n",
    "    train_parse[i, 4] = len(tokenizer_stem_nostop(train_parse[i, 1]))\n",
    "\n",
    "pd.DataFrame(data=train_parse, columns=['title', 'content', 'author', 'n_fig', 'n_words']).to_csv('train_b_parsed.csv')\n",
    "\n",
    "train_parse = np.zeros((df_test.values.shape[0], 5),dtype=object)\n",
    "\n",
    "#for i in range(5):\n",
    "\n",
    "for i in range(df_test['Page content'].values.shape[0]):\n",
    "    train_parse[i, 0], train_parse[i, 1], train_parse[i, 2],  train_parse[i, 3] \\\n",
    "            = procd.parse_html(df_test['Page content'][i])\n",
    "    train_parse[i, 4] = len(tokenizer_stem_nostop(train_parse[i, 1]))\n",
    "\n",
    "pd.DataFrame(data=train_parse, columns=['title', 'content', 'author','n_fig', 'n_words']).to_csv('test_b_parsed.csv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_parsed.csv')\n",
    "df_test = pd.read_csv('test_parsed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['app store', 'black friday', 'bowl ad', 'break bad', 'comic con', 'digit media', 'episod recap', 'first time', 'game throne', 'googl doodl', 'googl glass', 'grumpi cat', 'hong kong', 'ipad mini', 'jimmi fallon', 'look like', 'march mad', 'mark zuckerberg', 'miley cyru', 'music video', 'need know', 'new york', 'new york citi', 'new york time', 'news need', 'news need know', 'north korea', 'oculu rift', 'rais million', 'real life', 'real time', 'report say', 'samsung galaxi', 'san francisco', 'small busi', 'social media', 'social network', 'star war', 'steve job', 'street view', 'super bowl', 'super bowl ad', 'taylor swift', 'tim cook', 'time laps', 'top tech', 'tv show', 'twitter account', 'video game', 'video recap', 'viral video', 'viral video recap', 'white hous', 'wi fi', 'world cup', 'world first', 'xbox one', 'xp xp', 'xp xp xp', 'xp xp xp xp', 'xp xp xp xp xp', 'year old', 'york citi', 'york time']\n"
     ]
    }
   ],
   "source": [
    "print(tit_tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 12)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "train_weekday = pd.read_csv('train_weekday.csv').values\n",
    "train_time = pd.read_csv('train_time.csv').values\n",
    "train_channel = pd.read_csv('train_channel.csv').values\n",
    "#poly = PolynomialFeatures(degree=3)\n",
    "#train_weekday = poly.fit_transform(train_weekday)\n",
    "#train_time = poly.fit_transform(train_time)\n",
    "#print(df['n_words'].shape)\n",
    "#X_train_aug = hstack([train_time.reshape(-1,1), df['n_fig'].reshape(-1,1)])\n",
    "#X_train_aug = train_weekday\n",
    "X_train_aug = np.concatenate((train_channel, train_weekday, train_time), axis=1)\n",
    "#X_train_aug = train_weekday * train_time\n",
    "#sc_x = StandardScaler(with_mean=False)\n",
    "#X_train_aug = sc_x.fit_transform(X_train_aug)\n",
    "print(X_train_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# hold out testing set\n",
    "y_train = df['Popularity']\n",
    "X_htrain, X_htest, y_htrain, y_htest = train_test_split(X_train_aug, y_train, test_size=0.3, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\"\"\"\n",
    "SGD = SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
    "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
    "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
    "       penalty='elasticnet', power_t=0.5, random_state=0, shuffle=True,\n",
    "       verbose=0, warm_start=False)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "SGD.fit(X_htrain, y_htrain)\n",
    "pred = SGD.predict(X_htrain)\n",
    "pred_t = SGD.predict(X_htest)\n",
    "\n",
    "score_train = metrics.accuracy_score(y_htrain, pred)\n",
    "score_test = metrics.accuracy_score(y_htest, pred_t)\n",
    "\n",
    "print(\"SGD score (train): %f, (test): %f\"%(score_train, score_test))\n",
    "ada = AdaBoostClassifier(base_estimator=SGD, algorithm='SAMME', n_estimators=20)\n",
    "ada = ada.fit(X_htrain, y_htrain)\n",
    "pred = ada.predict(X_htrain)\n",
    "pred_t = ada.predict(X_htest)\n",
    "\n",
    "score_train = metrics.accuracy_score(y_htrain, pred)\n",
    "score_test = metrics.accuracy_score(y_htest, pred_t)\n",
    "\n",
    "print(\"SGD score (train): %f, (test): %f\"%(score_train, score_test))\n",
    "\n",
    "\"\"\"\n",
    "SGD = RandomForestClassifier(n_estimators=500, random_state=0, bootstrap=True, n_jobs=-1)\n",
    "bag = BaggingClassifier(base_estimator=SGD, n_estimators=100, \n",
    "                        max_samples=0.7, bootstrap=True,\n",
    "                        max_features=1.0, bootstrap_features=False, \n",
    "                        n_jobs=-1, random_state=1)\n",
    "bag.fit(X_htrain, y_htrain)\n",
    "\n",
    "pred = bag.predict(X_htrain)\n",
    "score_train = metrics.accuracy_score(y_htrain, pred)\n",
    "pred = bag.predict(X_htest)\n",
    "score_test = metrics.accuracy_score(y_htest, pred)\n",
    "\n",
    "print(\"SGD score (train): %f, (test): %f\"%(score_train, score_test))\n",
    "candidates = []\n",
    "candidates.append({'clf':SGD, 'C':1, 'score':1.0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_weekday = pd.read_csv('test_weekday.csv').values\n",
    "test_time = pd.read_csv('test_time.csv').values\n",
    "\n",
    "X_test_con = hstack([X_test, test_weekday, test_time, df_test['n_fig'].reshape(-1,1)])\n",
    "#X_test_con = np.concatenate((test_weekday, test_time), axis=1)\n",
    "#X_test_con = test_weekday * test_time\n",
    "#print(X_test_con.shape)\n",
    "#X_test_std = sc_x.fit_transform(X_test_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for cand in candidates:\n",
    "    predict = cand['clf'].predict(X_test_con)\n",
    "    output = np.zeros((X_test_con.shape[0], 2), dtype=int)\n",
    "    output[:, 0] = df_test['Id']\n",
    "    output[:, 1] = predict\n",
    "    df_output = pd.DataFrame(data=output, columns=['Id', 'Popularity'])\n",
    "    df_output.to_csv('test___%f.csv' % (cand['C']), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ML_ENV]",
   "language": "python",
   "name": "conda-env-ML_ENV-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
